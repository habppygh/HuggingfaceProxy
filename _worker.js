/**
 * é…ç½®åŒºåŸŸ
 * è¯·åŠ¡å¿…ä¿®æ”¹ä¸ºä½ å®é™…ç»‘å®šçš„åŸŸå
 */
const MAIN_SUBDOMAIN = 'hf';             // ä½ çš„ä¸»å…¥å£å‰ç¼€ (å¯¹åº” hf.yourdomain.com)

// hf_downloader.py è„šæœ¬å†…å®¹æ¨¡æ¿
const HF_DOWNLOADER_SCRIPT = `#!/usr/bin/env python3
"""
Hugging Face æ–‡ä»¶ä¸‹è½½å™¨
é€šè¿‡ä»£ç†æœåŠ¡å™¨ä¸‹è½½ Hugging Face ä»“åº“æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
    python hf_downloader.py <repo_id> [é€‰é¡¹]
    
ç¤ºä¾‹:
    python hf_downloader.py bert-base-uncased
    python hf_downloader.py openai/whisper-large-v3 --type model
    python hf_downloader.py bigcode/starcoder --revision main --workers 8
"""

import argparse
import os
import sys
import json
import hashlib
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from urllib.parse import urljoin, quote
from typing import Optional, List, Dict, Any
from dataclasses import dataclass
from tqdm import tqdm

try:
    import requests
except ImportError:
    print("è¯·å…ˆå®‰è£… requests: pip install requests")
    sys.exit(1)

# ============== é…ç½® ==============
# æ³¨æ„: é€šè¿‡ https://xx.xxx.com/hf_downloader.py ä¸‹è½½æ—¶ï¼Œ
# Worker ä¼šè‡ªåŠ¨å°†ä¸‹é¢çš„åŸŸåæ›¿æ¢ä¸ºè¯·æ±‚çš„åŸŸå
PROXY_DOMAIN = "{{PROXY_DOMAIN}}"  # ä½ çš„ä»£ç†åŸŸå
MAX_RETRIES = 3                    # æœ€å¤§é‡è¯•æ¬¡æ•°
CHUNK_SIZE = 8 * 1024 * 1024       # 8MB æ¯å—
DEFAULT_WORKERS = 4                # é»˜è®¤å¹¶è¡Œä¸‹è½½æ•°


@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯"""
    path: str           # ç›¸å¯¹è·¯å¾„
    size: int           # æ–‡ä»¶å¤§å° (bytes)
    oid: str            # æ–‡ä»¶ OID (ç”¨äº LFS)
    lfs: bool           # æ˜¯å¦æ˜¯ LFS æ–‡ä»¶
    download_url: str   # ä¸‹è½½åœ°å€


class HFDownloader:
    """Hugging Face ä¸‹è½½å™¨"""
    
    def __init__(
        self,
        repo_id: str,
        repo_type: str = "model",
        revision: str = "main",
        output_dir: Optional[str] = None,
        proxy_domain: str = PROXY_DOMAIN,
        workers: int = DEFAULT_WORKERS,
        token: Optional[str] = None
    ):
        self.repo_id = repo_id
        self.repo_type = repo_type
        self.revision = revision
        self.proxy_domain = proxy_domain
        self.workers = workers
        self.token = token or os.environ.get("HF_TOKEN")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir:
            self.output_dir = Path(output_dir)
        else:
            # é»˜è®¤ä½¿ç”¨ä»“åº“åä½œä¸ºç›®å½•
            safe_name = repo_id.replace("/", "_")
            self.output_dir = Path.cwd() / safe_name
            
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºåŸºç¡€ URL
        self.base_url = f"https://{proxy_domain}"
        
        # API è·¯å¾„å‰ç¼€
        if repo_type == "dataset":
            self.api_prefix = f"/api/datasets/{repo_id}"
            self.download_prefix = f"/datasets/{repo_id}/resolve/{revision}"
        elif repo_type == "space":
            self.api_prefix = f"/api/spaces/{repo_id}"
            self.download_prefix = f"/spaces/{repo_id}/resolve/{revision}"
        else:  # model
            self.api_prefix = f"/api/models/{repo_id}"
            self.download_prefix = f"/{repo_id}/resolve/{revision}"
        
        # Session é…ç½®
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "HF-Downloader/1.0 (Python)"
        })
        if self.token:
            self.session.headers["Authorization"] = f"Bearer {self.token}"
    
    def get_file_list(self) -> List[FileInfo]:
        """è·å–ä»“åº“ä¸­æ‰€æœ‰æ–‡ä»¶çš„åˆ—è¡¨"""
        url = f"{self.base_url}{self.api_prefix}/tree/{self.revision}"
        
        print(f"ğŸ“‚ æ­£åœ¨è·å–æ–‡ä»¶åˆ—è¡¨: {url}")
        
        all_files = []
        self._fetch_tree_recursive("", all_files)
        
        print(f"âœ… å…±å‘ç° {len(all_files)} ä¸ªæ–‡ä»¶")
        return all_files
    
    def _fetch_tree_recursive(self, path: str, files: List[FileInfo]) -> None:
        """é€’å½’è·å–ç›®å½•æ ‘"""
        params = {"recursive": "true"} if not path else {}
        
        if path:
            url = f"{self.base_url}{self.api_prefix}/tree/{self.revision}/{path}"
        else:
            url = f"{self.base_url}{self.api_prefix}/tree/{self.revision}"
            params["recursive"] = "true"
        
        try:
            resp = self.session.get(url, params=params, timeout=30)
            resp.raise_for_status()
            items = resp.json()
            
            for item in items:
                if item.get("type") == "file":
                    file_path = item["path"]
                    size = item.get("size", 0)
                    oid = item.get("oid", "")
                    lfs = item.get("lfs") is not None
                    
                    # æ„å»ºä¸‹è½½ URL
                    encoded_path = quote(file_path, safe="/")
                    download_url = f"{self.base_url}{self.download_prefix}/{encoded_path}"
                    
                    files.append(FileInfo(
                        path=file_path,
                        size=size,
                        oid=oid,
                        lfs=lfs,
                        download_url=download_url
                    ))
                    
        except requests.RequestException as e:
            print(f"âš ï¸ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    def download_file(self, file_info: FileInfo, progress_bar: Optional[tqdm] = None) -> bool:
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
        output_path = self.output_dir / file_info.path
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸”å¤§å°ç›¸åŒ
        if output_path.exists() and output_path.stat().st_size == file_info.size:
            if progress_bar:
                progress_bar.update(file_info.size)
            return True
        
        # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
        resume_pos = 0
        if output_path.exists():
            resume_pos = output_path.stat().st_size
        
        for attempt in range(MAX_RETRIES):
            try:
                headers = {}
                if resume_pos > 0:
                    headers["Range"] = f"bytes={resume_pos}-"
                
                resp = self.session.get(
                    file_info.download_url,
                    headers=headers,
                    stream=True,
                    timeout=60,
                    allow_redirects=True
                )
                
                # å¤„ç†é‡å®šå‘åçš„å“åº”
                if resp.status_code == 416:  # Range Not Satisfiable - æ–‡ä»¶å·²å®Œæ•´
                    if progress_bar:
                        progress_bar.update(file_info.size - resume_pos)
                    return True
                    
                resp.raise_for_status()
                
                # ç¡®å®šå†™å…¥æ¨¡å¼
                mode = "ab" if resume_pos > 0 and resp.status_code == 206 else "wb"
                if mode == "wb":
                    resume_pos = 0  # é‡æ–°ä¸‹è½½
                
                with open(output_path, mode) as f:
                    for chunk in resp.iter_content(chunk_size=CHUNK_SIZE):
                        if chunk:
                            f.write(chunk)
                            if progress_bar:
                                progress_bar.update(len(chunk))
                
                return True
                
            except Exception as e:
                print(f"\\nâš ï¸ ä¸‹è½½å¤±è´¥ ({attempt + 1}/{MAX_RETRIES}): {file_info.path} - {e}")
                if attempt < MAX_RETRIES - 1:
                    import time
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        return False
    
    def download_all(self, files: Optional[List[FileInfo]] = None) -> Dict[str, Any]:
        """ä¸‹è½½æ‰€æœ‰æ–‡ä»¶"""
        if files is None:
            files = self.get_file_list()
        
        if not files:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
            return {"success": 0, "failed": 0, "skipped": 0}
        
        # è®¡ç®—æ€»å¤§å°
        total_size = sum(f.size for f in files)
        print(f"\\nğŸ“¦ å‡†å¤‡ä¸‹è½½ {len(files)} ä¸ªæ–‡ä»¶, æ€»å¤§å°: {self._format_size(total_size)}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ”§ å¹¶è¡Œæ•°: {self.workers}\\n")
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        print("=" * 60)
        print(f"{'æ–‡ä»¶å':<45} {'å¤§å°':>12}")
        print("=" * 60)
        for f in files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            name = f.path if len(f.path) <= 45 else "..." + f.path[-42:]
            print(f"{name:<45} {self._format_size(f.size):>12}")
        if len(files) > 10:
            print(f"... è¿˜æœ‰ {len(files) - 10} ä¸ªæ–‡ä»¶")
        print("=" * 60 + "\\n")
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress = tqdm(
            total=total_size,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
            desc="ä¸‹è½½è¿›åº¦"
        )
        
        results = {"success": 0, "failed": 0, "failed_files": []}
        lock = threading.Lock()
        
        def download_task(file_info: FileInfo) -> bool:
            success = self.download_file(file_info, progress)
            with lock:
                if success:
                    results["success"] += 1
                else:
                    results["failed"] += 1
                    results["failed_files"].append(file_info.path)
            return success
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œä¸‹è½½
        with ThreadPoolExecutor(max_workers=self.workers) as executor:
            futures = [executor.submit(download_task, f) for f in files]
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"\\nâŒ ä»»åŠ¡å¼‚å¸¸: {e}")
        
        progress.close()
        
        # æ‰“å°ç»“æœ
        print("\\n" + "=" * 60)
        print(f"âœ… ä¸‹è½½å®Œæˆ: {results['success']}/{len(files)} ä¸ªæ–‡ä»¶æˆåŠŸ")
        if results["failed"] > 0:
            print(f"âŒ å¤±è´¥æ–‡ä»¶: {results['failed']} ä¸ª")
            for f in results["failed_files"]:
                print(f"   - {f}")
        print("=" * 60)
        
        return results
    
    @staticmethod
    def _format_size(size: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ["B", "KB", "MB", "GB", "TB"]:
            if size < 1024:
                return f"{size:.2f} {unit}"
            size /= 1024
        return f"{size:.2f} PB"


def main():
    parser = argparse.ArgumentParser(
        description="é€šè¿‡ä»£ç†ä¸‹è½½ Hugging Face ä»“åº“æ–‡ä»¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    %(prog)s bert-base-uncased
    %(prog)s openai/whisper-large-v3 --type model
    %(prog)s bigcode/starcoder --revision main --workers 8
    %(prog)s microsoft/phi-2 --output ./my_models
        """
    )
    
    parser.add_argument("repo_id", help="ä»“åº“ ID (ä¾‹å¦‚: bert-base-uncased æˆ– openai/whisper-large-v3)")
    parser.add_argument("--type", "-t", choices=["model", "dataset", "space"], 
                        default="model", help="ä»“åº“ç±»å‹ (é»˜è®¤: model)")
    parser.add_argument("--revision", "-r", default="main", 
                        help="åˆ†æ”¯/ç‰ˆæœ¬ (é»˜è®¤: main)")
    parser.add_argument("--output", "-o", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--workers", "-w", type=int, default=DEFAULT_WORKERS,
                        help=f"å¹¶è¡Œä¸‹è½½æ•° (é»˜è®¤: {DEFAULT_WORKERS})")
    parser.add_argument("--proxy", "-p", default=PROXY_DOMAIN,
                        help=f"ä»£ç†åŸŸå (é»˜è®¤: {PROXY_DOMAIN})")
    parser.add_argument("--token", help="Hugging Face Token (ä¹Ÿå¯è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡)")
    parser.add_argument("--list-only", "-l", action="store_true",
                        help="ä»…åˆ—å‡ºæ–‡ä»¶ï¼Œä¸ä¸‹è½½")
    
    args = parser.parse_args()
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ¤— Hugging Face ä»£ç†ä¸‹è½½å™¨                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ä»“åº“: {args.repo_id:<53} â•‘
â•‘  ç±»å‹: {args.type:<53} â•‘
â•‘  åˆ†æ”¯: {args.revision:<53} â•‘
â•‘  ä»£ç†: {args.proxy:<53} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    downloader = HFDownloader(
        repo_id=args.repo_id,
        repo_type=args.type,
        revision=args.revision,
        output_dir=args.output,
        proxy_domain=args.proxy,
        workers=args.workers,
        token=args.token
    )
    
    if args.list_only:
        files = downloader.get_file_list()
        print("\\nğŸ“‹ æ–‡ä»¶åˆ—è¡¨:")
        print("=" * 70)
        for f in files:
            lfs_tag = "[LFS]" if f.lfs else ""
            print(f"{f.path:<50} {downloader._format_size(f.size):>12} {lfs_tag}")
        print("=" * 70)
        print(f"æ€»è®¡: {len(files)} ä¸ªæ–‡ä»¶, {downloader._format_size(sum(f.size for f in files))}")
    else:
        downloader.download_all()


if __name__ == "__main__":
    main()
`;

export default {
    async fetch(request, env, ctx) {
        const url = new URL(request.url);
        const hostname = url.hostname;

        // è‡ªåŠ¨è·å–ä¸»åŸŸå (å‡è®¾ hostname æ ¼å¼ä¸º prefix.root_domain)
        const firstDotIndex = hostname.indexOf('.');
        const MY_ROOT_DOMAIN = firstDotIndex !== -1 ? hostname.substring(firstDotIndex + 1) : hostname;

        // å¤„ç† /hf_downloader.py è¯·æ±‚ - åŠ¨æ€ç”Ÿæˆè„šæœ¬
        if (url.pathname === '/hf_downloader.py') {
            const script = HF_DOWNLOADER_SCRIPT.replace('{{PROXY_DOMAIN}}', hostname);
            return new Response(script, {
                status: 200,
                headers: {
                    'Content-Type': 'text/x-python; charset=utf-8',
                    'Content-Disposition': 'attachment; filename="hf_downloader.py"',
                    'Cache-Control': 'no-cache'
                }
            });
        }

        // 1. è§£æå½“å‰è¯·æ±‚çš„ç›®æ ‡ (Upstream)
        let upstreamHost = '';

        // æå–å­åŸŸåéƒ¨åˆ† (ä¾‹å¦‚: cas-bridge_xethub)
        // é€»è¾‘ï¼šå–ç¬¬ä¸€ä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†
        const prefix = firstDotIndex !== -1 ? hostname.substring(0, firstDotIndex) : '';

        if (prefix === MAIN_SUBDOMAIN) {
            // ä¸»å…¥å£ -> huggingface.co
            upstreamHost = 'huggingface.co';
        } else {
            // CDN æ˜ å°„é€»è¾‘:
            // 1. å°† --- è¿˜åŸä¸ºç‚¹ . (cas-bridge---xethub -> cas-bridge.xethub)
            // 2. è¡¥å…¨ .hf.co åç¼€
            upstreamHost = prefix.replace(/---/g, '.') + '.hf.co';
        }

        // 2. æ„å»ºå‘å¾€æºç«™çš„è¯·æ±‚
        url.hostname = upstreamHost;
        url.protocol = 'https:';

        const newRequest = new Request(url, {
            method: request.method,
            headers: request.headers,
            body: request.body,
            redirect: 'manual' // ã€å…³é”®ã€‘æ‰‹åŠ¨æ‹¦æˆª 302 é‡å®šå‘
        });

        // å¼ºåˆ¶è¦†ç›– Host å¤´ï¼Œç¡®ä¿æºç«™èƒ½å¤„ç†
        newRequest.headers.set('Host', upstreamHost);

        try {
            // 3. å‘èµ·è¯·æ±‚
            const response = await fetch(newRequest);

            // 4. æ‹¦æˆªå¹¶é‡å†™é‡å®šå‘ (301, 302, 307 ç­‰)
            if ([301, 302, 303, 307, 308].includes(response.status)) {
                const location = response.headers.get('Location');
                if (location) {
                    try {
                        const locUrl = new URL(location);
                        const locHost = locUrl.hostname;
                        let newPrefix = '';
                        let shouldRewrite = false;

                        // åˆ¤æ–­é‡å®šå‘çš„ç›®æ ‡åœ°å€
                        if (locHost === 'huggingface.co') {
                            // å¦‚æœè·³å›ä¸»ç«™
                            newPrefix = MAIN_SUBDOMAIN;
                            shouldRewrite = true;
                        } else if (locHost.endsWith('.hf.co')) {
                            // å¦‚æœè·³å¾€ CDN (å¦‚ cas-bridge.xethub.hf.co)
                            // é€»è¾‘: å»æ‰ .hf.co -> å°†ç‚¹ . æ›¿æ¢ä¸º ---
                            const rawPrefix = locHost.slice(0, -6); // ç§»é™¤ ".hf.co"
                            newPrefix = rawPrefix.replace(/\./g, '---');
                            shouldRewrite = true;
                        }

                        // å¦‚æœéœ€è¦é‡å†™ Location
                        if (shouldRewrite) {
                            // æ„é€ æ–°çš„é‡å®šå‘åœ°å€æŒ‡å‘ä½ çš„åŸŸå
                            locUrl.hostname = `${newPrefix}.${MY_ROOT_DOMAIN}`;
                            locUrl.protocol = 'https:'; // ä¿æŒ HTTPS

                            // å¤åˆ¶å¹¶ä¿®æ”¹å“åº”å¤´
                            const newHeaders = new Headers(response.headers);
                            newHeaders.set('Location', locUrl.toString());

                            return new Response(response.body, {
                                status: response.status,
                                statusText: response.statusText,
                                headers: newHeaders
                            });
                        }
                    } catch (e) {
                        console.error("Location parse error:", e);
                    }
                }
            }

            // 5. éé‡å®šå‘è¯·æ±‚ï¼Œç›´æ¥è¿”å›æ•°æ®
            return response;

        } catch (e) {
            return new Response(`Proxy Error: ${e.message}`, { status: 502 });
        }
    }
};